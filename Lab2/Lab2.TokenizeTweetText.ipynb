{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.3: Tweet pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'JordiTorresBCN', ':', 'Just', 'an', 'Example', '!', ':', 'D', 'http', ':', '//fib.upc.edu', '#', 'masterMEI']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet = 'RT @JordiTorresBCN: Just an Example! :D http://fib.upc.edu #masterMEI'\n",
    "\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweetTokens = preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@JordiTorresBCN', ':', 'Just', 'an', 'Example', '!', ':D', 'http://fib.upc.edu', '#masterMEI']\n"
     ]
    }
   ],
   "source": [
    "print tweetTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it has been processed, we can remove the punctuation and the stop words of the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@JordiTorresBCN', 'Just', 'Example', ':D', 'http://fib.upc.edu', '#masterMEI']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def cleanTweet(tweetTokens):\n",
    "    tweetTokens_noPunctutation = [w for w in tweetTokens if w not in string.punctuation]\n",
    "    tweetTokens_cleaned = [w for w in tweetTokens_noPunctutation if not w in stopwords.words('english')]\n",
    "    return tweetTokens_cleaned\n",
    "\n",
    "cleanedTweet = cleanTweet(tweetTokens)\n",
    "print cleanedTweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to find out the name of the people mentioned in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  RT/NNP\n",
      "  @JordiTorresBCN/NNP\n",
      "  Just/NNP\n",
      "  Example/NNP\n",
      "  :D/NNP\n",
      "  http://fib.upc.edu/NN\n",
      "  #masterMEI/NNP\n",
      "  (PERSON Jordi/NNP Torres/NNP)\n",
      "  B/NNP\n",
      "  C/NNP\n",
      "  N/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "def discoverProfile(tweet):\n",
    "    tweetNewDataList = ([re.findall('[A-Z][^A-Z]*', w) for w in tweet if w[0] is \"@\"])\n",
    "    tweetWithUsers = tweet\n",
    "    if tweetNewDataList: tweetWithUsers += tweetNewDataList[0]\n",
    "    taggedTweet = ne_chunk(pos_tag(tweetWithUsers))\n",
    "    return taggedTweet\n",
    "\n",
    "print discoverProfile(cleanedTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyse another tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Strong', 'comes', 'in', 'three', 'colors', '!', 'Which', '#OnePlus3T', 'Otterbox', 'is', 'your', 'favorite', '?', 'http://onepl.us/3OB']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tweet = 'Strong comes in three colors! Which #OnePlus3T Otterbox is your favorite? http://onepl.us/3OB'\n",
    "\n",
    "tweetTokens = preprocess(tweet)\n",
    "print tweetTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Strong', 'comes', 'three', 'colors', 'Which', '#OnePlus3T', 'Otterbox', 'favorite', 'http://onepl.us/3OB']\n"
     ]
    }
   ],
   "source": [
    "cleanedTweet = cleanTweet(tweetTokens)\n",
    "print cleanedTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Strong/JJ)\n",
      "  comes/VBZ\n",
      "  three/CD\n",
      "  colors/NNS\n",
      "  (PERSON Which/NNP)\n",
      "  #OnePlus3T/VBD\n",
      "  (PERSON Otterbox/NNP)\n",
      "  favorite/JJ\n",
      "  http://onepl.us/3OB/NN)\n"
     ]
    }
   ],
   "source": [
    "print discoverProfile(cleanedTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this Python library tagging task (*Named Entity Recognition*) does not work with all kind of tweets, like for the previous one. It seems that it simply tags as proper noun all that words it doesn't know or recognize within the context of the sentece, for instance, or something similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
