{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As in the previous lab, authentication keys are read from a JSON file (which is not commited in the repository):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "jsonFileText = None\n",
    "with open(\"TwitterKeys.json\", 'r') as inputFile: jsonFileText = inputFile.read().replace('\\n', '')\n",
    "keys = json.loads(jsonFileText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = keys[\"consumer_key\"]\n",
    "consumer_secret = keys[\"consumer_secret\"]\n",
    "access_token = keys[\"access_token\"]\n",
    "access_secret = keys[\"access_secret\"]\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate json file from the keyword defined here.\n",
    "The filter(...) method will select the word or the line that you define here. The amount of time allowed to the download has barely been 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "topic = \"Manchester\"    # Change only this line to generate a file for this topic and with this name\n",
    "fileName = topic + \".json\"\n",
    "\n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open(fileName, 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the 'text' field from the file that was generated in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json  \n",
    "with open(fileName,'r') as json_file:\n",
    "         for line in json_file:\n",
    "             tweet = json.loads(line)\n",
    "             if \"text\" in tweet: print tweet[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to print the apropriate tokenization according to Twitter styles (as seen in Lab 2) we need to define de following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the tokens for tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(fileName, 'r') as f:\n",
    "#import io\n",
    "#f=io.open('data/stream_barcelona.json', 'r', encoding='utf8' )\n",
    "     for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        if \"text\" in tweet: \n",
    "            tokens = preprocess(tweet['text'])\n",
    "            print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the most popular tweets in that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(fileName, 'r') as f:\n",
    "#import io\n",
    "#f=io.open('data/stream_barcelona.json', 'r', encoding='utf8' )\n",
    "     for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        if \"text\" in tweet: \n",
    "            tokens = preprocess(tweet['text'])\n",
    "            print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the common stop words used in Twitter, like 'RT' (re-tweet), via (from)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\") # download the stopword corpus on our computer\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'RT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the most common terms without considering the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "with open(fileName, 'r') as f:\n",
    "    count_all_stop = Counter()\n",
    "    count_all_hash = Counter()\n",
    "    count_all_only = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        if \"text\" in tweet: \n",
    "            terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "            terms_hash = [term for term in preprocess(tweet['text']) if term.startswith('#')]\n",
    "            terms_only = [term for term in preprocess(tweet['text']) if term not in stop and not term.startswith(('#', '@'))]\n",
    "            \n",
    "            count_all_stop.update(terms_stop)\n",
    "            count_all_hash.update(terms_hash)\n",
    "            count_all_only.update(terms_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting all type of terms\n",
    "We can notice some strange symbols in the most common terms (emoticons maybe?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word, index in count_all_stop.most_common(5): print '%s : %s' % (word, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couting just hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word, index in count_all_hash.most_common(5): print '%s : %s' % (word, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couting only terms (no hashtags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word, index in count_all_only.most_common(5): print '%s : %s' % (word, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the most popular words:\n",
    "This will search in our file and print the 10 most popular words from the JSON file. 10 different words and the repetition up to 15 times.\n",
    "We could fathom what recently happened in Manchester. Basically we obtain terms like (What?) 'bombing', 'ISIS', (Where?) 'Manchester', 'ManchesterArena', (Event?) 'ArianaGrande'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "with open(fileName, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        if \"text\" in tweet: \n",
    "            terms_hash = [term for term in preprocess(tweet['text']) if term.startswith('#') and term not in stop]        \n",
    "            count_all.update(terms_hash)\n",
    "# Print the first 10 most frequent words\n",
    "print(count_all.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a bar chart from the previously selected words:\n",
    "Of course, the most popular words/terms/hashtags are, by far, Manchester and derivates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_x, sorted_y = zip(*count_all.most_common(15))\n",
    "#print(sorted_x, sorted_y)\n",
    "\n",
    "plt.bar(range(len(sorted_x)), sorted_y, width=0.75, align='center');\n",
    "plt.xticks(range(len(sorted_x)), sorted_x, rotation=80);\n",
    "plt.axis('tight'); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
